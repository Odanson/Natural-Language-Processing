{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOiYI32-eUfF"
      },
      "source": [
        "# Week 9: Named Entity Recognition\n",
        "\n",
        "This week we are going to looking at named entity recognition in the fiction genre. In doing so we will introduce the spaCy library (https://spacy.io/) which provides a number of very fast, state-of-the-art accuracy tools for carrying out NLP tasks including part-of-speech tagging, dependency parsing and named entity recognition.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raM1LTb5eUfG"
      },
      "source": [
        "#preliminary imports\n",
        "\n",
        "from google.colab import drive\n",
        "#mount google drive\n",
        "drive.mount('/content/drive/')\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/NLE Notebooks/resources/')\n",
        "\n",
        "import pandas as pd\n",
        "import operator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW_tPwNseUfG"
      },
      "source": [
        "## Project Gutenberg\n",
        "\n",
        "[Project Gutenberg electronic text archive](http://www.gutenberg.org/) contains around 25,000 free electronic books.\n",
        "\n",
        "A small selection is made available through the NLTK. For the full list, run the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrud69QPe3Qr"
      },
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K_tAJUFeUfG"
      },
      "source": [
        "from nltk.corpus import gutenberg\n",
        "gutenberg.fileids()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZO6Uhx0eUfI"
      },
      "source": [
        "We can get the raw text of any of the novels using the `gutenberg.raw(fileid)` method.  This returns a String."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK4JmiOGeUfI"
      },
      "source": [
        "emma=gutenberg.raw('austen-emma.txt')\n",
        "emma[:500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDC653sjeUfJ"
      },
      "source": [
        "Now, we carry out a little bit of cleaning of the text.  Check you understand what each line in the `clean_text()` function does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Vwu-DWIeUfJ"
      },
      "source": [
        "import re\n",
        "def clean_text(astring):\n",
        "    #replace newlines with space\n",
        "    newstring=re.sub(\"\\n\",\" \",astring)\n",
        "    #remove title and chapter headings\n",
        "    newstring=re.sub(\"\\[[^\\]]*\\]\",\" \",newstring)\n",
        "    newstring=re.sub(\"VOLUME \\S+\",\" \",newstring)\n",
        "    newstring=re.sub(\"CHAPTER \\S+\",\" \",newstring)\n",
        "    newstring=re.sub(\"\\s\\s+\",\" \",newstring)\n",
        "    #return re.sub(\"([^\\.|^ ])  +\",r\"\\1 .  \",newstring).lstrip().rstrip()\n",
        "    return newstring.lstrip().rstrip()\n",
        "\n",
        "clean_emma=clean_text(emma)\n",
        "print(len(emma))\n",
        "print(len(clean_emma))\n",
        "clean_emma[:500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZw5tkm0eUfJ"
      },
      "source": [
        "## SpaCy\n",
        "\n",
        "If working at home, you may need to install spaCy and download a set of English models.  at the command line:\n",
        "\n",
        "```\n",
        "pip install spacy\n",
        "python -m spacy download en_core_web_sm\n",
        "```\n",
        "\n",
        "In the lab, or once you have done this at home, you should then be able to set up a spaCy processing pipeline as follows. If working on colab than this should work automatically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuku0GwWeUfJ"
      },
      "source": [
        "import spacy\n",
        "#nlp=spacy.load('en')\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "type(nlp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9FHGRuzeUfJ"
      },
      "source": [
        "Now we can run any text string through the language processing pipeline stored in `nlp`\n",
        "This next cell might take a few minutes to run since it carries out all of the SpaCy NLP functionality on the input text.  It will return a SpaCy `Doc` object which contains the text plus various annotations.  See the SpaCy documentation https://spacy.io/api/doc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z26NL3teUfJ"
      },
      "source": [
        "nlp_emma=nlp(clean_emma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AjnNopNeUfJ"
      },
      "source": [
        "type(nlp_emma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM26L9EeeUfJ"
      },
      "source": [
        "For example, we can now iterate over sentences in the document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M1o8boAeUfJ"
      },
      "source": [
        "for s in nlp_emma.sents:\n",
        "    print(s)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naVVmUxeeUfJ"
      },
      "source": [
        "We can iterate over tokens in sentences and find out the labels added by SpaCy to each token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zXgZk99eUfJ"
      },
      "source": [
        "emma_sents=list(nlp_emma.sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9amnqyVeUfK"
      },
      "source": [
        "print(emma_sents[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT25iErbeUfK"
      },
      "source": [
        "def display_sent(asent):\n",
        "    headings=[\"token\",\"lower\",\"lemma\",\"pos\",\"NER\"]\n",
        "    info=[]\n",
        "    for t in asent:\n",
        "        info.append([t.text,t.lower_,t.lemma_,t.pos_,t.ent_type_])\n",
        "    return(pd.DataFrame(info,columns=headings))\n",
        "        \n",
        "display_sent(emma_sents[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhdIUCsceUfK"
      },
      "source": [
        "### Exercise 1.1\n",
        "Run the `display_sent()` function on each of the first ten sentences of Emma (as stored in `emma_sents`).\n",
        "* What errors do you see in the named entity recognition?\n",
        "* Can you see any patterns in the words, lemmas or part-of-speech tags which might be used to improve the named entity recognition on these sentences?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZqWA-PeeUfK"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZivY8DSmeUfK"
      },
      "source": [
        "### Exercise 1.2\n",
        "Write a function 'make_tag_lists()' which takes a list of sentences as input and which returns 3 lists:\n",
        "1. tokens\n",
        "2. POS tags\n",
        "3. Named Entity tags\n",
        "\n",
        "These lists should be the same length (189191, if applied to the all of the sentences in `nlp_emma`) and maintain the order of the text, i.e., position i in each list should refer to the same token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxADffMReUfK"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AWz7fRgeUfK"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUWG_kcTeUfK"
      },
      "source": [
        "### Exercise 1.3\n",
        "Write a function `extract_entities` which takes a list of tokens, a list of tags and a tag-type and returns a dictionary of all of the **chunks** which have the given tag-type; together with their frequency in the text.\n",
        "\n",
        "You can assume that two consecutive tokens with the same tag are part of the same chunk.\n",
        "\n",
        "Test your code and you should get the following output (for the given input):\n",
        "\n",
        "```python\n",
        "[ ] extract_entities(toks,ner,\"PERSON\")\n",
        "```\n",
        "\n",
        "```\n",
        "    {':-- Robert Martin': 1,\n",
        "     ';\"--': 1,\n",
        "     'A. W. \"': 1,\n",
        "     'Absurd': 1,\n",
        "     'Adair': 1,\n",
        "     'Anne': 1,\n",
        "     'Anne Cox': 2,\n",
        "     ...\n",
        "```\n",
        "\n",
        "This tells us that \"Anne Cox\" is tagged twice as a named entity of type \"PERSON\" in the text.  How many occurrences of \"Miss Woodhouse\" tagged as a \"PERSON\" are there?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN70pZKneUfK"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIZGru5LeUfK"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISRNuVVPeUfK"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfoTGHdPeUfK"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx75rl5UeUfK"
      },
      "source": [
        "### Exercise 1.4\n",
        "Use your code to find \n",
        "* the 20 most commonly referred to people in Emma\n",
        "* the 20 most commonly referred to places in Emma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMu4JvTPeUfK"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXKOF2xMeUfK"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHRM_BgmeUfK"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqEDPhBueUfL"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyvwBWq_eUfL"
      },
      "source": [
        "### Exercise 1.5\n",
        "Look at the lists of people and places generated.  Assuming no knowledge of the characters and plot of Emma, what errors can you see?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qsv0lqWeUfL"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvzZC6QkeUfL"
      },
      "source": [
        "## Extensions\n",
        "\n",
        "Code one or more of the following extensions.  In all cases, compare the lists of most frequently occurring named entities generated with the original ones.\n",
        "\n",
        "### Expanding NER Chunks\n",
        "* if the word immediately before or after a named entity chunk is POS-tagged as a PROPN, assume that this word is also part of the named entity chunk\n",
        "\n",
        "For example, where the token \"Miss\" has pos-tag \"PROPN\" and is immediately followed by a token labelled with \"PERSON\", then it should also be labelled with \"PERSON\". \n",
        "\n",
        "### Relabelling NER Chunks\n",
        "* if a named entity occurs more frequently elsewhere in the text as a different type, assume that it has been mis-labelled here\n",
        "\n",
        "For example, all 9 occurrences of \"Jane Fairfax\" labelled as \"GPE\" could be relabelled as \"PERSON\".\n",
        "\n",
        "### Linking NEs\n",
        "* find candidates for named entity linking.  \n",
        "\n",
        "For example, \"Churchill\" and \"Frank Churchill\" and \"Frank\" might all refer to the same person.\n",
        "However, you should proceed with care.  Anyone who knows the story well would tell you that \"Knightley\" and \"John Knightley\" do not refer to the same character (they are brothers).  As a further extension, give your linking functionality access to a list of known characters e.g., from https://www.janeausten.org/emma/cast-of-characters.asp\n",
        "\n",
        "### Co-occurring NEs\n",
        "* find NEs that tend to co-occur together.\n",
        "\n",
        "Can you find pairs of named entities which often occur together (or even better, occur more often together than one would expect if named entities occur independently)?  You could consider pairs of people or alternatively co-occurrences of people and places.\n",
        "\n",
        "### NEs over Time\n",
        "* record the position in the text of each named entity occurrence\n",
        "* make a plot showing how the amount of occurrences of a given named entity varies with position in the text\n",
        "\n",
        "If you store each text position in `list_of_indices`, you could use:\n",
        "`pd.Series(np.histogram(list_of_indices, bins=num_bins)` to help you with this\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mCHxegseUfL"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}