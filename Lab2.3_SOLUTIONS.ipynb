{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ef0a3a",
   "metadata": {
    "colab_type": "text",
    "id": "NpWT3jBdPsyb"
   },
   "source": [
    "## DIY Tokenisation with Regular Expressions (EXTENSION)\n",
    "Text doesn't come in neat tokens ready for analysis, it must first undergo sentence segmentation and tokenisation.  \n",
    "The nltk package can handle sentence segmentation and word tokenisation of a corpus for you.\n",
    "The word tokeniser in NLTK is based on regular expressions.  For a deeper understanding, work through this extension lab where you will build your own regular expression based tokeniser and compare it with the NLTK implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fb7ce9",
   "metadata": {
    "colab_type": "text",
    "id": "V-fartdXPsyc"
   },
   "source": [
    "### Making your own tokeniser\n",
    "In this section, you will write your own Python function, which takes as input a single string representing a sentence, and returns a <b>list of strings</b> obtained by splitting the sentence into tokens.\n",
    "\n",
    "Let's start by simply splitting by whitespace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f61c67fc",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "--W0Zm2TPsyc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'is', 'the', 'air-speed', 'velocity', 'of', 'an', 'unladen', 'swallow?']\n"
     ]
    }
   ],
   "source": [
    "print(\"   What    is the    air-speed   velocity of  an unladen swallow?   \".split()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371dba85",
   "metadata": {
    "colab_type": "text",
    "id": "7w2WIe4XPsyf"
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "- In the empty code cell below write a [function](http://docs.python.org/tutorial/controlflow.html#defining-functions), `tokenise` which takes a sentence as input and returns a list of the tokens making up the sentence. Your first version of this function should tokenise only on whitespace, as shown in the cell above. Show that your function works on the sentence shown above.\n",
    "- Note: this is intended to be an easy exercise - just a couple of lines of code - don't overcomplicate it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d55d6e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 885,
     "status": "ok",
     "timestamp": 1600521185029,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "6vx1MRpkPsyf",
    "outputId": "11a52284-4786-45d7-b8b3-3a078203b995"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What',\n",
       " 'is',\n",
       " 'the',\n",
       " 'air-speed',\n",
       " 'velocity',\n",
       " 'of',\n",
       " 'an',\n",
       " 'unladen',\n",
       " 'swallow?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenise(sentence):\n",
    "    return sentence.split()\n",
    "\n",
    "s=\"   What    is the    air-speed   velocity of  an unladen swallow?   \"\n",
    "tokenise(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aaa64b",
   "metadata": {
    "colab_type": "text",
    "id": "814FI2TqPsyh"
   },
   "source": [
    "### Exercise 2.2\n",
    "\n",
    "- In the empty code cell below write code that applies your tokenise function to each sentence in a sample of 30 sentences taken from  the twitter_samples corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afd39e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the sample_sentences() function from the last lab will be useful here\n",
    "#next week we will look at including useful functions like this in a utils python file which we can import from\n",
    "import random\n",
    "\n",
    "def sample_sentences(corpus,sample_size):\n",
    "    \n",
    "    size=len(corpus)\n",
    "    ids=random.sample(range(size),sample_size) \n",
    "    sample=[corpus[i] for i in ids]  \n",
    "    return sample\n",
    "\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "raw_sents=sample_sentences(twitter_samples.strings(),30)\n",
    "tokenised=[tokenise(s) for s in raw_sents]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58c832a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8495,
     "status": "ok",
     "timestamp": 1600521213225,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "aczjgkPXPsyi",
    "outputId": "2666a437-1ccf-40d6-9abc-87b929e340b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Time', 'to', 'watch', 'nigel', 'farage'],\n",
       " ['I', 'WANT', 'TO', 'WATCH!', ':(', 'https://t.co/wdETpUIyOZ'],\n",
       " ['Indeed!',\n",
       "  'Ran',\n",
       "  'out',\n",
       "  'of',\n",
       "  'characters,',\n",
       "  'knew',\n",
       "  \"you'd\",\n",
       "  'get',\n",
       "  'it',\n",
       "  ':)',\n",
       "  'https://t.co/KfLhKQfwho'],\n",
       " ['RT',\n",
       "  '@jreynoldsMP:',\n",
       "  'Missed',\n",
       "  'the',\n",
       "  'start.',\n",
       "  'Did',\n",
       "  'Cameron',\n",
       "  'also',\n",
       "  'bring',\n",
       "  'a',\n",
       "  'note',\n",
       "  'reminding',\n",
       "  'everyone',\n",
       "  'Tories',\n",
       "  'were',\n",
       "  'pledged',\n",
       "  'to',\n",
       "  'match',\n",
       "  \"Lab's\",\n",
       "  'spending',\n",
       "  'plans',\n",
       "  'pre-f…'],\n",
       " ['@xemig', 'sounds', 'still', 'like', 'a', 'good', 'idea', ':)'],\n",
       " ['@cyrenity',\n",
       "  'I',\n",
       "  'know',\n",
       "  'he',\n",
       "  'is,',\n",
       "  'only',\n",
       "  'saying',\n",
       "  'what',\n",
       "  'I',\n",
       "  'know',\n",
       "  ':)'],\n",
       " ['@Munchbunchsdad',\n",
       "  '@LouiseMensch',\n",
       "  'the',\n",
       "  'cash-in-hand',\n",
       "  'brigade',\n",
       "  'have',\n",
       "  'always',\n",
       "  'voted',\n",
       "  'Tory.'],\n",
       " ['@svaertathel',\n",
       "  'HAHAHAHA',\n",
       "  'no',\n",
       "  'way',\n",
       "  ':p',\n",
       "  'what',\n",
       "  'must',\n",
       "  'we',\n",
       "  'wear',\n",
       "  'there',\n",
       "  'ah?',\n",
       "  'school',\n",
       "  'uniform?'],\n",
       " ['RT',\n",
       "  '@IndyForTheGuy:',\n",
       "  '\"Vote',\n",
       "  'for',\n",
       "  'me',\n",
       "  'or',\n",
       "  'I',\n",
       "  'will',\n",
       "  'go',\n",
       "  'in',\n",
       "  'such',\n",
       "  'a',\n",
       "  'huff.',\n",
       "  'And',\n",
       "  'while',\n",
       "  'I',\n",
       "  'sulk,',\n",
       "  'the',\n",
       "  'tories',\n",
       "  'will',\n",
       "  'get',\n",
       "  'back',\n",
       "  'in,',\n",
       "  '&amp;',\n",
       "  'then',\n",
       "  \"you'll\",\n",
       "  'be',\n",
       "  'sorry!\"',\n",
       "  'http:/…'],\n",
       " ['seolhyun',\n",
       "  'isnt',\n",
       "  'in',\n",
       "  'these',\n",
       "  'first',\n",
       "  'eps',\n",
       "  'because',\n",
       "  'shes',\n",
       "  'filming',\n",
       "  'a',\n",
       "  'drama',\n",
       "  ':(('],\n",
       " ['RT',\n",
       "  '@fertile_shire:',\n",
       "  'Voting',\n",
       "  'Tory?',\n",
       "  'Or',\n",
       "  'ukip?',\n",
       "  'http://t.co/lKvbPOKKiY'],\n",
       " ['@BenedictNichol1',\n",
       "  '@RedScareBot',\n",
       "  'Farage',\n",
       "  'thinks',\n",
       "  'that',\n",
       "  'we',\n",
       "  'still',\n",
       "  \"haven't\",\n",
       "  'recovered',\n",
       "  'from',\n",
       "  'being',\n",
       "  'behind',\n",
       "  'the',\n",
       "  '\"iron',\n",
       "  'curtain\".',\n",
       "  'Well,',\n",
       "  'I',\n",
       "  'have.'],\n",
       " ['@_blacktomorrow',\n",
       "  'im',\n",
       "  'so',\n",
       "  'jealous',\n",
       "  'we',\n",
       "  'always',\n",
       "  'planned',\n",
       "  'to',\n",
       "  'go',\n",
       "  'together!!!',\n",
       "  ':('],\n",
       " ['His',\n",
       "  'pal',\n",
       "  '#EdMiliband',\n",
       "  'had',\n",
       "  'a',\n",
       "  'real',\n",
       "  'stumbling',\n",
       "  'time',\n",
       "  'https://t.co/uVURM6a0uM'],\n",
       " ['@JaredLeto', 'Wheeeeeeeeeeeeeeeen?!?!?!', ':D'],\n",
       " ['RT',\n",
       "  '@364690:',\n",
       "  'Ed',\n",
       "  'Miliband',\n",
       "  'the',\n",
       "  'Frank',\n",
       "  'Spencer',\n",
       "  'of',\n",
       "  'politics.'],\n",
       " ['@UKIP',\n",
       "  '100%',\n",
       "  'of',\n",
       "  'Britons',\n",
       "  'have',\n",
       "  'brains',\n",
       "  'and',\n",
       "  'thats',\n",
       "  'why',\n",
       "  'they',\n",
       "  'will',\n",
       "  'not',\n",
       "  'vote',\n",
       "  'ukip'],\n",
       " ['@JPonpolitics',\n",
       "  'Why',\n",
       "  'are',\n",
       "  'you',\n",
       "  'talking',\n",
       "  'about',\n",
       "  'Miliband???',\n",
       "  'He',\n",
       "  'was',\n",
       "  'killed',\n",
       "  'by',\n",
       "  'the',\n",
       "  'audience,',\n",
       "  'Nigel',\n",
       "  'had',\n",
       "  'the',\n",
       "  'audience',\n",
       "  'on',\n",
       "  'his',\n",
       "  'side',\n",
       "  'after',\n",
       "  '5',\n",
       "  'minutes!'],\n",
       " ['#PKwalaSawaal',\n",
       "  'when',\n",
       "  'will',\n",
       "  'India',\n",
       "  'become',\n",
       "  'a',\n",
       "  'developed',\n",
       "  'nation',\n",
       "  ':(',\n",
       "  '?',\n",
       "  '@SonyMAX'],\n",
       " ['@lauraewaddell',\n",
       "  '@ScotlandTonight',\n",
       "  '@bbcqt',\n",
       "  'that',\n",
       "  'party',\n",
       "  'of',\n",
       "  'social',\n",
       "  'democracy',\n",
       "  'gave',\n",
       "  'us',\n",
       "  'the',\n",
       "  'Tories',\n",
       "  'in',\n",
       "  '1979'],\n",
       " ['RT',\n",
       "  '@robbersdog58:',\n",
       "  '@mkpdavies',\n",
       "  \"Didn't\",\n",
       "  'faze',\n",
       "  'him',\n",
       "  'at',\n",
       "  'all.',\n",
       "  'They',\n",
       "  \"couldn't\",\n",
       "  'barrage',\n",
       "  'the',\n",
       "  'Farage.'],\n",
       " ['RT',\n",
       "  '@Tommy_Colc:',\n",
       "  'Financial',\n",
       "  'Times',\n",
       "  'come',\n",
       "  'out',\n",
       "  'in',\n",
       "  'support',\n",
       "  'of',\n",
       "  'Tories',\n",
       "  'claiming',\n",
       "  'Miliband',\n",
       "  'is',\n",
       "  '\"preoccupied',\n",
       "  'w/',\n",
       "  'inequality\".',\n",
       "  'The',\n",
       "  'man',\n",
       "  'who',\n",
       "  'wrote',\n",
       "  'it',\n",
       "  'http:/…'],\n",
       " ['@za5',\n",
       "  'they',\n",
       "  \"can't\",\n",
       "  'even',\n",
       "  'deliver',\n",
       "  'ice',\n",
       "  'cream.',\n",
       "  'I',\n",
       "  'tried',\n",
       "  'twice,',\n",
       "  'all',\n",
       "  'the',\n",
       "  'ice',\n",
       "  'cream',\n",
       "  'vans',\n",
       "  'are',\n",
       "  'busy',\n",
       "  ':('],\n",
       " ['RT',\n",
       "  '@kdugdalemsp:',\n",
       "  '.',\n",
       "  '@Ed_Miliband',\n",
       "  'has',\n",
       "  'called',\n",
       "  'the',\n",
       "  \"SNP's\",\n",
       "  'bluff.',\n",
       "  'Big',\n",
       "  'question',\n",
       "  'for',\n",
       "  'Nicola',\n",
       "  'Sturgeon',\n",
       "  'now',\n",
       "  'is',\n",
       "  'will',\n",
       "  'she',\n",
       "  'oppose',\n",
       "  \"Labour's\",\n",
       "  'programme',\n",
       "  'and',\n",
       "  '…'],\n",
       " ['RT',\n",
       "  '@abstex:',\n",
       "  'The',\n",
       "  'FT',\n",
       "  'is',\n",
       "  'backing',\n",
       "  'the',\n",
       "  'Tories.',\n",
       "  'On',\n",
       "  'an',\n",
       "  'unrelated',\n",
       "  'note,',\n",
       "  \"here's\",\n",
       "  'a',\n",
       "  'photo',\n",
       "  'of',\n",
       "  'FT',\n",
       "  'leader',\n",
       "  'writer',\n",
       "  'Jonathan',\n",
       "  'Ford',\n",
       "  '(next',\n",
       "  'to',\n",
       "  'Boris)',\n",
       "  'http://t.c…'],\n",
       " ['ed', 'miliband', 'trrympst', 'solo'],\n",
       " ['RT',\n",
       "  '@WalesForYES:',\n",
       "  'If',\n",
       "  'Labour',\n",
       "  'let',\n",
       "  'the',\n",
       "  'Tories',\n",
       "  'back',\n",
       "  'into',\n",
       "  'power,',\n",
       "  'Wales',\n",
       "  'will',\n",
       "  'never',\n",
       "  'forgive',\n",
       "  'them.',\n",
       "  '#GE2015'],\n",
       " ['@Shewho13',\n",
       "  'TFW',\n",
       "  'no',\n",
       "  'boyfriend',\n",
       "  ':(',\n",
       "  '(lol',\n",
       "  'I',\n",
       "  'jest)',\n",
       "  'cooking',\n",
       "  'relaxes',\n",
       "  'me.',\n",
       "  \"I'm\",\n",
       "  'sure',\n",
       "  \"I'll\",\n",
       "  'eat',\n",
       "  'it',\n",
       "  'for',\n",
       "  'lunch',\n",
       "  'tomorrow.'],\n",
       " ['RT',\n",
       "  '@maitlis:',\n",
       "  'This',\n",
       "  'is',\n",
       "  'a',\n",
       "  'completely',\n",
       "  'different',\n",
       "  'dynamic',\n",
       "  'between',\n",
       "  '#bbcqt',\n",
       "  'audience',\n",
       "  'and',\n",
       "  'Farage',\n",
       "  '.',\n",
       "  'They',\n",
       "  'seem',\n",
       "  'to',\n",
       "  'be',\n",
       "  'feeding',\n",
       "  'him',\n",
       "  'his',\n",
       "  'lines,',\n",
       "  'not',\n",
       "  'challen…'],\n",
       " ['Ahhh', 'Food', 'Poisoning', 'is', 'so', 'prevalent...', ':(']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d329bb",
   "metadata": {
    "colab_type": "text",
    "id": "8AKKbp68Psyk"
   },
   "source": [
    "In most tokenisation policies (e.g. in the Wall Street Journal corpus), contractions like \"I'm\" tend to be split into \"I\" and \"'m\".  \n",
    "\n",
    "When it comes to more than just splitting by whitespace, it can be convenient to use [regular expressions](http://docs.python.org/library/re.html) to process the string in some way. The following code cell illustrates this. Trying running it and then read on to discover how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2736002",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 707,
     "status": "ok",
     "timestamp": 1600521239760,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "FIjpz1ekPsyl",
    "outputId": "6ec5dbc2-eb3e-4d75-944a-7c263290ae99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', \"'re\", 'using', 'coconuts', '!']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "print(re.sub(\"([.?!'])\", \" \\g<1>\", \"You're using coconuts!\").split())   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452985b8",
   "metadata": {
    "colab_type": "text",
    "id": "dSG_oL86Psyn"
   },
   "source": [
    "Let's look at how the above code works by breaking it down.  \n",
    "\n",
    "First, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "173524d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 861,
     "status": "ok",
     "timestamp": 1600521241275,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "hM5-SQZ3Psyo",
    "outputId": "ca2d39de-946f-4c65-a662-1f5aa88b13b8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You 're using coconuts!\n"
     ]
    }
   ],
   "source": [
    "print(re.sub(\"'\", \" '\", \"You're using coconuts!\")   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fde148c",
   "metadata": {
    "colab_type": "text",
    "id": "7YPMAAxfPsyq"
   },
   "source": [
    "As you can see, this code takes the string \"You're using coconuts!\" and inserts a space before the apostophe, the `'` character. \n",
    "\n",
    "Let's see how it works...\n",
    "\n",
    "The first argument of `re.sub`, i.e. `\"'\"`, is a regular expression that in this case is extremely simple, since it only matches the apostophe character, `'`.\n",
    "\n",
    "The second argument of `re.sub`, where we see `\" '\"`, indicates that an apostophe should be substituted by a space followed by an apostophe.\n",
    "\n",
    "Now let's make it slightly more complicated. We also want to insert a space before the `\"!\"`, so let's look at how to do that. \n",
    "\n",
    "Run the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a30853cd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 653,
     "status": "ok",
     "timestamp": 1600521245999,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "ZsvQ-hyEPsyq",
    "outputId": "d7966b78-9cbc-4e04-ea74-cc447c7433d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You 're using coconuts !\n"
     ]
    }
   ],
   "source": [
    "print(re.sub(\"(['!])\", \" \\g<1>\", \"You're using coconuts!\")   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cd1773",
   "metadata": {
    "colab_type": "text",
    "id": "IvO0UTN2Psyt"
   },
   "source": [
    "The first argument of `re.sub`, has been changed to `\"(['!])\"`, which is a regular expression that matches either an apostophe character,`'`, or an exclamation mark,`!`.\n",
    "\n",
    "This is achieved with the regular expression `\"['!]\"`, where the square brackets enclose the alternative characters. \n",
    "\n",
    "Why does the regular expression contain parenthesis? \n",
    "\n",
    "It has to do with what we need to put as the second argument of `re.sub` where the substitution is specified. \n",
    "\n",
    "To understand this, you need to appreciate that we want to add a space before an apostrophe and also a space before an exclamation mark. How can we specify that in the second argument of `re.sub`? \n",
    "\n",
    "The answer is that we need to make use of the the idea of a **group**.\n",
    "\n",
    "The parenthesis in `\"(['!])\"` define the start and end of a group. In this case the whole regular expression is a group. In general, however, there can be several sets of parenthesis defining several groups. For example, the regular expression `\"([Tt]h)e (m*n)\"` has two groups. Groups are numbered from left to right, so the group in the regular expression `\"(['!])\"` is group 1. \n",
    "\n",
    "Defining this group allows us to refer to the string that matches the regular expression `\"(['!])\"`, which will be either an apostrophe or an exclamation mark. This is then used in the second argument of `re.sub`, where we see `\" \\g<1>\"`, which indicates that the material that matches the apostophe or exclamation mark should be substituted by a space followed by the symbol that was matched. The `1` in `\\g<1>` tells us that it is group one.\n",
    "\n",
    "We are now ready to look at the original code, which is reproduced below and should now make sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2b4d1c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 711,
     "status": "ok",
     "timestamp": 1600521253564,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "X0bKL4OjPsyt",
    "outputId": "f3076b3e-f385-4cde-9516-f80e9d5a5053"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', \"'re\", 'using', 'coconuts', '!']\n"
     ]
    }
   ],
   "source": [
    "print(re.sub(\"([.?!'])\", \" \\g<1>\", \"You're using coconuts!\").split())   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b65c575",
   "metadata": {
    "colab_type": "text",
    "id": "hoIfNfXrPsyv"
   },
   "source": [
    "First, the spaces are added before any full stop, question mark, exclamation mark or apostrophe.\n",
    "The resulting string is then split on white space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13919ef0",
   "metadata": {
    "colab_type": "text",
    "id": "AVT1ao7wPsyw"
   },
   "source": [
    "### Exercise 2.3\n",
    "\n",
    "- Write a new version of your `tokenise` function that uses `re.sub` in the way we've just considered. Make sure you test your new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3ca0342",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 819,
     "status": "ok",
     "timestamp": 1600521269011,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "Km5UPznUPsyw",
    "outputId": "0aa0bfca-1aa9-49c9-8c9d-4333a2418316"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What',\n",
       " 'is',\n",
       " 'the',\n",
       " 'air-speed',\n",
       " 'velocity',\n",
       " 'of',\n",
       " 'an',\n",
       " 'unladen',\n",
       " 'swallow',\n",
       " '?']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenise(sent):\n",
    "    return re.sub(\"([.?!'])\", \" \\g<1>\", sent).split()\n",
    "\n",
    "tokenise(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa69497",
   "metadata": {
    "colab_type": "text",
    "id": "YHIflXwvPsyz"
   },
   "source": [
    "### Exercise 2.4\n",
    "\n",
    "\n",
    "- In an empty code cell below, extend your tokeniser function to cater for the following guidelines. \n",
    "- Test out your new tokeniser on the string  \n",
    "`\"After saying \\\"I won't help, I'm gonna leave!\\\", on his parents' arrival, the boy's behaviour improved.\"`  \n",
    " notice that the `\"` characters in the test sentence have been espaced, appearing as `\\\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3bebca",
   "metadata": {
    "colab_type": "text",
    "id": "I9L_dHIwPsyz"
   },
   "source": [
    "### Guidelines\n",
    "\n",
    "- punctuation is split from adjoining words\n",
    "- opening double quotes are changed to two single forward quotes.\n",
    "- closing double quotes are changed to two single backward quotes.\n",
    "- the Anglo-Saxon genitive of nouns are split into their component parts.\n",
    "  - e.g. `\"children's\"` produces `\"children 's\"`\n",
    "  - e.g. `\"parents'\"` produces `\"parents '\"`\n",
    "- contractions should be split into component parts\n",
    "  - e.g. `\"won't\"` produces `\"wo n't\"`\n",
    "  - e.g. `\"gonna\"` produces `\"gon na\"`\n",
    "  - e.g. `\"I'm\"` produces `\"I 'm\"`\n",
    "  \n",
    "  \n",
    "These tokenisation guidelines are a subset of those found at\n",
    "ftp://ftp.cis.upenn.edu/pub/treebank/public_html/tokenization.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ede86a3",
   "metadata": {
    "colab_type": "text",
    "id": "yparu947Psy0"
   },
   "source": [
    "\n",
    "### Hints:\n",
    "\n",
    "- Use multiple calls to `re.sub` to deal with different cases one at a time. As in...\n",
    "\n",
    "```\n",
    "    sentence = re.sub(<pattern1>, <replacement1>,sentence)\n",
    "    sentence = re.sub(<pattern2>, <replacement2>,sentence)\n",
    "    sentence = re.sub(<pattern3>, <replacement3>,sentence)\n",
    "```\n",
    "\n",
    "- Order your calls to `re.sub` so that you deal with the specific cases first and the more general cases later.\n",
    "\n",
    "- In dealing with the replacement of start and end `\"`, you will find the following useful:\n",
    "\n",
    ">The `'*'`, `'+'`, and `'?'` qualifiers are all *greedy*; they match\n",
    ">as much text as possible.  Sometimes this behaviour isn't desired; if the RE\n",
    ">`<.\\*>` is matched against `<a> b <c>`, it will match the entire\n",
    ">string, and not just `<a>`.  Adding `'?'` after the qualifier makes it\n",
    ">perform the match in *non-greedy* or *minimal* fashion; as *few*\n",
    ">characters as possible will be matched.  Using the RE `<.\\*?>` will match\n",
    ">only `<a>`.  \n",
    "(taken from https://docs.python.org/2/library/re.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a7272d5",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 705,
     "status": "ok",
     "timestamp": 1600521290628,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "9W8H1LCHPsy0"
   },
   "outputs": [],
   "source": [
    "def tokenise(sentence):\n",
    "    sentence = re.sub(\"'(s|m|(re)|(ve)|(ll)|(d))\\s\", \" '\\g<1> \",sentence + \" \")\n",
    "    sentence = re.sub(\"s'\\s\", \"s ' \",sentence)\n",
    "    sentence = re.sub(\"n't\\s\", \" n't \",sentence)\n",
    "    sentence = re.sub(\"gonna\", \"gon na\",sentence)\n",
    "    sentence = re.sub(\"\\\"(.+?)\\\"\", \"`` \\g<1> ''\",sentence)   \n",
    "    sentence = re.sub(\"([.,?!])\", \" \\g<1> \", sentence)\n",
    "    return sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b629f86b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 658,
     "status": "ok",
     "timestamp": 1600521304763,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "VzGRhNbyPsy2",
    "outputId": "eee284f2-1c15-4c85-82d4-854db31cb8bb",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['After',\n",
       " 'saying',\n",
       " '``',\n",
       " 'I',\n",
       " 'wo',\n",
       " \"n't\",\n",
       " 'help',\n",
       " ',',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'gon',\n",
       " 'na',\n",
       " 'leave',\n",
       " '!',\n",
       " \"''\",\n",
       " ',',\n",
       " 'on',\n",
       " 'his',\n",
       " 'parents',\n",
       " \"'\",\n",
       " 'arrival',\n",
       " ',',\n",
       " 'the',\n",
       " 'boy',\n",
       " \"'s\",\n",
       " 'behaviour',\n",
       " 'improved',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testsent=\"After saying \\\"I won't help, I'm gonna leave!\\\", on his parents' arrival, the boy's behaviour improved.\"\n",
    "tokenise(testsent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3e3928",
   "metadata": {},
   "source": [
    "### Exercise 3.1\n",
    "\n",
    "- In the code cell below write code to run both the `NLTK_Tokenise` and your `tokenise` function on a sample of 10 sentences from the twitter_samples corpus.\n",
    "- Compare the output.  What differences do you notice?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "206b89a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NLTK</th>\n",
       "      <th>MINE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT</td>\n",
       "      <td>RT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@</td>\n",
       "      <td>@blairmcdougall:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blairmcdougall</td>\n",
       "      <td>Nicola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>:</td>\n",
       "      <td>Sturgeon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nicola</td>\n",
       "      <td>being</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>//t.co/pNq95QKn9G</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>-</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>:</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>-</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>(</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>214 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  NLTK              MINE\n",
       "0                   RT                RT\n",
       "1                    @  @blairmcdougall:\n",
       "2       blairmcdougall            Nicola\n",
       "3                    :          Sturgeon\n",
       "4               Nicola             being\n",
       "..                 ...               ...\n",
       "209  //t.co/pNq95QKn9G              None\n",
       "210                  -              None\n",
       "211                  :              None\n",
       "212                  -              None\n",
       "213                  (              None\n",
       "\n",
       "[214 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "from itertools import zip_longest\n",
    "\n",
    "\n",
    "sample=sample_sentences(twitter_samples.strings(),10)\n",
    "nltk_toks=[]\n",
    "my_toks=[]\n",
    "for s in sample:\n",
    "    nltk_toks += word_tokenize(s) # run the nltk tokeniser\n",
    "    my_toks += tokenise(s) # run your tokeniser\n",
    "\n",
    "pd.DataFrame(list(zip_longest(nltk_toks,my_toks)),columns=[\"NLTK\", \"MINE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a83977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
