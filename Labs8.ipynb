{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNr7m4n3Fvu7"
      },
      "source": [
        "# Lab Week 8: Part-of-Speech Tagging \n",
        "\n",
        "This week we are learning about part-of-speech (POS) tagging.  This involves deciding the correct part-of speech tag (e.g., noun, verb, adjective etc) for each word in a sentence.  Since the correct tag for each word depends not only on the current word but on the tags of those words around it, it is generally viewed as a **sequence labelling** problem.  In other words, for a given sequence of words, we are asking what is the most likely sequence of tags?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avA_7Rf2Fvu9"
      },
      "outputs": [],
      "source": [
        "###mount google drive\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import sys\n",
        "import operator\n",
        "#make sure you append the path where your utils.py file is.\n",
        "sys.path.append('/content/drive/My Drive/NLE Notebooks/Week4LabsSolutions/')\n",
        "sys.path.append('/Users/juliewe/Documents/teaching/NLE/NLE2021/w4/Week4LabsSolutions/')\n",
        "from utils import *\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyHtJ98mFvvD"
      },
      "source": [
        "## Average PoS tag ambiguity \n",
        "The Part-of-Speech (PoS) tag ambiguity of a word type is a measure of how varied the PoS tags are for that type.   Note that here, we talk about the ambiguity of a word type rather than a word token because any given token has a single tag but different occurrences of the same type may have different tags.  For example, some occurrences of the word *bank* have the tag *noun* whereas others have the tag *verb*\n",
        "\n",
        "Some types are always (or almost always) labelled with the same PoS tag, so exhibit no (or very little) ambiguity. It is easy to predict the correct PoS tag for such words. \n",
        "\n",
        "On the other hand, a type that is commonly labelled by a variety of different PoS tags exhibits a high level of ambiguity, and is more challenging to deal with.\n",
        "\n",
        "In this session, we are going to be considering two measures of a type's ambiguity. We will be using the Wall Street Journal corpus as it has been hand-annotated with part of speech tags.  A 10% sample of it is available via NLTK via the `treebank` corpus reader.   \n",
        "We will consider \n",
        "* a simple measure that just **counts** the number of different tags that label the type. \n",
        "* a more complex information-theoretic measure based on **entropy**.\n",
        "\n",
        "First, we can use the treebank's method `tagged_words()` to get a list of all tokens in the corpus tagged with their POS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_MZM9vmUjbA"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import treebank\n",
        "\n",
        "taggedWSJ=treebank.tagged_words()\n",
        "\n",
        "taggedWSJ[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeHgtOFrFvvT"
      },
      "source": [
        "### Exercise 1.1\n",
        "Write a function `find_tag_distributions(tokentaglist)` which finds the (frequency) distributions of tags for every word in the input.\n",
        "* input: a list of pairs (token,tag)\n",
        "* returns: a dictionary of dictionaries.  The key to the outermost dictionary should be the word and the key to each internal dictionary should be the tag.  The value associated with the tag in the internal dictionary should be its frequency of occurrence.\n",
        "\n",
        "Test your function on `taggedWSJ` and look at the tag distribution for the word `the`.   You should find that you get:\n",
        "\n",
        "`{DT: 4038, 'NNP':1, 'JJ':5, 'CD':1}`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIuDtQHOFvvU"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Gj0dZW1SXaY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QArRz4NYFvvy"
      },
      "source": [
        "### Exercise 1.2\n",
        "Write a function `simple_pos_ambiguity` which can take the tagged WSJ text and returns a dictionary containing the number of part of speech tags which each word type has.  Note that this is simply the length of the dictionary associated with that word in the output from `find_tag_distributions`.\n",
        "\n",
        "Check that you get the following results:\n",
        "the: 4\n",
        "white: 2\n",
        "show: 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wj-OMMgFvvz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSUNSo13Fvv2"
      },
      "source": [
        "### Exercise 1.3\n",
        "Find the mean average value of the `simple_pos_ambiguity` score for word types in the WSJ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kdfVMgBFvv2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em4JvkMTFvv5"
      },
      "source": [
        "## Entropy as a Measure of Tag Ambiguity\n",
        "\n",
        "**Entropy** is a measure of uncertainty. A word will have high entropy when it occurs the same number of times with each part of speech. There is maximum uncertainty as to which part of speech it has.\n",
        "\n",
        "The larger the part of speech tagset, the greater the potential for uncertainty, and the higher the entropy can be.\n",
        "\n",
        "In the cell below we see a function `entropy`. It's argument is a list of counts (which in our case are counts of how many times a word appeared with a given part of speech).\n",
        "\n",
        "Check that you understand how the code implements this definition of entropy:\n",
        "$$H([x_1,\\ldots,x_n])= - \\sum_{i=1}^nP(x_i)\\log_2 P(x_i)$$\n",
        "where $n$ is the number of PoS tags, and $x_i$ is a count of how many times the word was labelled with the $i$th PoS tag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fl2bLOJjFvv5"
      },
      "outputs": [],
      "source": [
        "def entropy(counts):            # counts = list of counts of occurrences of tags\n",
        "    total = sum(counts)         # get total number of occurrences\n",
        "    if not total: return 0      # if zero occurrences in total, then 0 entropy\n",
        "    entropy = 0\n",
        "    for i in counts:            # for each tag count\n",
        "        p = i/total      # probability that the token occurs with this tag\n",
        "        try:\n",
        "            entropy += p * math.log(p,2) # add to entropy\n",
        "        except ValueError: pass     # if p==0, then ignore this p\n",
        "    return -entropy if entropy else entropy   # only negate if nonzero, otherwise \n",
        "                                              # floats can return -0.0, which is weird.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZUPeuVuFvv9"
      },
      "source": [
        "### Exercise 2.1\n",
        "Experiment with the `entropy` function.\n",
        "- It takes a list of counts as its argument.\n",
        "- Compare the entropy of a list where all counts are the same with the entropy of a list of different counts.\n",
        "- See what happens when you vary the length of the list of counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4Lb79_qFvv9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xatJmrvU5WH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgjNBjOvFvwA"
      },
      "source": [
        "### Exercise 2.2\n",
        "Write a function `entropy_ambiguity` which takes the tagged WSJ text and returns a dictionary containing the entropy of each word.\n",
        "\n",
        "Test it out your function; you should find:\n",
        "\n",
        "`white: 0.91829\n",
        "show: 1.41955\n",
        "the: 0.02036`\n",
        "\n",
        "How does this correspond to our intuitions about which word types are more difficult to correctly POS tag?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vv7DgTJmFvwB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdE0Tk3dFvwE"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVla9DKcFvwF"
      },
      "source": [
        "## A Simple Unigram Tagger\n",
        "Now, we will be looking at part of speech tagging itself i.e., the problem of determining the correct tag for a given word token. We will\n",
        "\n",
        "* implement a unigram tagger\n",
        "* experiment with an off-the-shelf POS tagger which utilises information about the previous words or tags in the sequence.\n",
        "\n",
        "First, lets get some tagged text from the WSJ and split it into a training and a testing set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7QD05jqFvwF"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_train_test_pos(split=0.7):\n",
        "\n",
        "    taggedWSJ=treebank.tagged_words()\n",
        "    #we don't want to randomly select data because we need to preserve sequence information\n",
        "    #so we are just going to take the first part as training and the second as test\n",
        "    n=int(len(taggedWSJ)*split)\n",
        "    return taggedWSJ[:n],taggedWSJ[n:]\n",
        "\n",
        "train, test = get_train_test_pos(split=0.8)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycHhfUHwXGLq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk1z9qGbFvwI"
      },
      "source": [
        "Now, we build a unigram model of the tag distribution for each word type.  We use the `find_tag_distributions` function defined earlier and store the result in the variable `unigram_model`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHPFWWjBFvwJ"
      },
      "outputs": [],
      "source": [
        "unigram_model=find_tag_distributions(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTJHn-YZXeTr"
      },
      "outputs": [],
      "source": [
        "unigram_model.get('the',{})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVrnzCnTFvwM"
      },
      "source": [
        "### Exercise 3.1\n",
        "Write a `uni_pos_tag` function which takes:\n",
        "* a sequence of tokens \\[wordtoken1,wordtoken2, ....\\]\n",
        "* a unigram model (stored as a dictionary of dictionaries\n",
        "and returns:\n",
        "* a tagged sequence of tokens \\[(wordtoken1,tag1),(wordtoken2,tag2),....\\]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SIfLgGUFvwN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6qzxKZ4FvwQ"
      },
      "source": [
        "### Exercise 3.2\n",
        "Test that your function works on both the training data `train` and the testing data `test`.  Remember, you can separate the tokens and the tags into two separate lists using:\n",
        "* `train_toks,train_tags=zip(*train)`\n",
        "* `test_toks,test_tags=zip(*test)`\n",
        "\n",
        "Don't worry about evaluating the accuracy at this point (that's the next exercise) - just check that you can generate sequences of (token,tag) pairs in both cases.  What happens if there is a word in the test data that didn't occur in the training data?  You might need to update your `uni_pos_tag` function to take this into account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9D6cNoDFvwQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Vep-r8lFvwW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xev9H6blFvwY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tF6Y9JiRFvwc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jTQsUrlFvwe"
      },
      "source": [
        "### Exercise 3.3\n",
        "Write a function `evaluate_uni_pos_tag` which will calculate the accuracy of the `uni_pos_tag` function. This should have as arguments:\n",
        "* the unigram_model\n",
        "* the gold standard sequence of (token,tag) pairs for comparison\n",
        "\n",
        "You should find that it is 94.6% accurate on the training data.  How accurate is it on the test data? \n",
        "\n",
        "As an extension, you could implement a uni_pos_tagger class, which combines the all of the functionality above, and then provide an `evaluate` function which evaluates a tagger. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFmQ_ZgjFvwf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnAThHYEFvwh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olWTComLFvwk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85cP6GboFvwm"
      },
      "source": [
        "## Beyond Unigram Tagging\n",
        "State-of-the-art POS-taggers use information about likely sequences of tags to get higher performance.\n",
        "\n",
        "In the lectures, we discussed the theory of Hidden Markov Model (HMM) tagging.  Here we are going to experiment with the HMM tagger available in NLTK.  First, however, we need to segment our sequence of (token,tag) pairs into a collection of shorter sequences (which roughly correspond with sentences).   This is because the Viterbi algorithm will try to find the best sequence of tags by considering the whole sequence.  Therefore, we want to reduce the maximum length of sequences for both efficiency and accuracy reasons  We are just going to split the sequence on every token which is tagged as a full stop. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCg0MAUXUjbK"
      },
      "outputs": [],
      "source": [
        "def sentence_split(labelledsequence):\n",
        "    #this is going to do a very rough job - just splitting on '.'\n",
        "    #due to the tags, we can't rejoin, use the sentence splitter and then re-tokenise\n",
        "    #we could do a better job than this, but don't really need to\n",
        "    #we just want to split the input up so that the HMM tagger doesn't view it as one long sequence which it needs to run Viterbi over\n",
        "    \n",
        "    sentences=[]\n",
        "    \n",
        "    sentence=[]\n",
        "    for token,tag in labelledsequence:\n",
        "        sentence.append((token,tag))\n",
        "        if tag=='.':\n",
        "            sentences.append(sentence)\n",
        "            sentence=[]\n",
        "    return sentences\n",
        "\n",
        "train_sentences=sentence_split(train)\n",
        "test_sentences=sentence_split(test)\n",
        "print(\"Number of training sentences: {}\".format(len(train_sentences)))\n",
        "print(\"Number of testing sentences: {}\".format(len(test_sentences)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21dyvh_LUjbK"
      },
      "source": [
        "We can now import the `nltk.tag.hmm` library.  To create a HiddenMarkovModelTagger, we first need a HiddenMarkovModelTrainer.  The train_supervised() method of this class will take the training sentences, estimate the emission and transition probabilities and return a HiddenMarkovModelTagger with these parameters.\n",
        "\n",
        "We can then run this HiddenMarkovModelTagger on unlabelled sequences using its `.tag()` method or test it on labelled sequences using its `.test()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7n1WRp33UjbK"
      },
      "outputs": [],
      "source": [
        "from nltk.tag import hmm\n",
        "hmmTrainer = hmm.HiddenMarkovModelTrainer([],[])\n",
        "hmmTagger =hmmTrainer.train_supervised(train_sentences)\n",
        "hmmTagger.test(train_sentences)\n",
        "hmmTagger.test(test_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRwofiYZUjbK"
      },
      "source": [
        "Note that the trained hmmTagger does really well at predicting the tags in the training sample but really badly at predicting the tags in the testing sample.  This is partly due to the small size of the training sample.  There are lots of tokens and tag transitions in the testing sample which haven't been seen before in the training sample.  We can improve this by smoothing the probability estimates.  By default, the `train_supervised` method uses a plain Maximum Likelihood Estimator to convert the observed frequency distributions into probability distributions.  However, we can pass it a different estimator which will carry out smoothing on the distributions.  Here we are going to use the LidstoneProbDist which will \"add-gamma\" to all of the counts (where gamma is a small number). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Wjs1PEKUjbL"
      },
      "outputs": [],
      "source": [
        "from nltk.probability import MLEProbDist,LidstoneProbDist\n",
        "\n",
        "#this is the default estimator used by HiddenMarkovModelTrainer.trained_supervised\n",
        "default_estimator = lambda fdist, bins: MLEProbDist(fdist,bins)\n",
        "\n",
        "#we are going to replace it with this\n",
        "gamma=1\n",
        "smoothed_estimator = lambda fdist, bins: LidstoneProbDist(fdist,gamma,bins)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TIUte1BUjbL"
      },
      "outputs": [],
      "source": [
        "hmmTagger_smooth =hmmTrainer.train_supervised(train_sentences,estimator=smoothed_estimator)\n",
        "hmmTagger_smooth.test(train_sentences)\n",
        "hmmTagger_smooth.test(test_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi6ud2rLUjbL"
      },
      "source": [
        "We can see that the accuracy on the training data has gone down but the accuracy on the testing data has improved a lot.  Smoothing appears to be helping, but maybe we can do better with a different value of gamma?\n",
        "\n",
        "First, we will need our own test function as unfortunately, hmmTagger.test() only prints the accuracy and does not return it for future use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRxydE88UjbL"
      },
      "outputs": [],
      "source": [
        "def evaluate(labelledsequences,tagger=hmmTagger):\n",
        "    count=0\n",
        "    correct=0\n",
        "    for sequence in labelledsequences:\n",
        "        goldtoks,goldtags=zip(*sequence)\n",
        "        goldtoks=list(goldtoks)\n",
        "        #print(goldtoks)\n",
        "        predictions=tagger.tag(goldtoks)\n",
        "        predtoks,predtags=zip(*predictions)\n",
        "        for g,p in zip(goldtags,predtags):\n",
        "            if g==p:\n",
        "                correct+=1\n",
        "            count+=1\n",
        "    return correct/count\n",
        "evaluate(test_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMch9tZ6UjbL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdUk7-QPUjbL"
      },
      "source": [
        "### Exercise 4\n",
        "\n",
        "Carry out an experiment to find the optimal value of gamma.  You should:\n",
        "    * experiment with different values of gamma using a training and validation set (created from the training set above)\n",
        "    * ideally average over a number of runs with different training, validation splits\n",
        "    * plot a graph of accuracy against gamma on both training and validation\n",
        "    * finally train a HmmTagger on all of the training data using the optimal value of gamma and evaluate its accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuTT80tvUjbL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbcXbgPBUjbL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqrNBkhZUjbM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVgaIT5uUjbM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67WLFOEvUjbM"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUe6HeWfUjbM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcT-LhWKFvwx"
      },
      "source": [
        "\n",
        "\n",
        "### Extension\n",
        "Find examples where the unigram tagger makes mistakes but the nltk hmm tagger is correct.  What different types of errors are being made?  Can you explain intuitively why the correct sequence predicted by the nltk hmm tagger is more likely than the one predicted by the unigram tagger?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIC5sMwjFvwy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kE63lP_Fvw0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVJV7Jk_Fvw4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBKDRkelc8_V"
      },
      "source": [
        "# Lecture Code for HMM Emission and Transition Probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qB3LMR1GFvw6"
      },
      "outputs": [],
      "source": [
        "def calculate_emissions(trainlist):\n",
        "    #trainlist is a list of (word,tag) pairs\n",
        "    emissions={}\n",
        "    for word,tag in trainlist:\n",
        "        current=emissions.get(tag,{})\n",
        "        current[word]=current.get(word,0)+1\n",
        "        emissions[tag]=current\n",
        "    return {tag:{word:value/sum(worddist.values()) for word,value in worddist.items()} \n",
        "            for tag,worddist in emissions.items()}\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_CK3JyKFvw8"
      },
      "outputs": [],
      "source": [
        "calculate_emissions(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KabkWZMyFvw_"
      },
      "outputs": [],
      "source": [
        "def calculate_transitions(trainlist):\n",
        "    transitions={}\n",
        "    previous=\"start\"\n",
        "    for _, tag in trainlist:\n",
        "        current=transitions.get(previous,{})\n",
        "        current[tag]=current.get(tag,0)+1\n",
        "        transitions[tag]=current\n",
        "        previous =tag\n",
        "    return {previous:{tag:value/sum(tagdist.values()) for tag,value in tagdist.items()} \n",
        "            for previous,tagdist in transitions.items()}\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4gMbmElFvxG"
      },
      "outputs": [],
      "source": [
        "calculate_transitions(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhUtBDXRFvxI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}